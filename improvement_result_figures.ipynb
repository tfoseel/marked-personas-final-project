{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marked word 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_THRESHOLD = 1.96 # Threshold for detecting marked words\n",
    "\n",
    "models = {\n",
    "    'gpt-3.5-turbo': 'data/gpt3_generations.csv',\n",
    "    'gpt-4-1106-preview': 'data/gpt4_generations.csv',\n",
    "    'CLOVA X': 'data/clovax_generations.csv',\n",
    "    'Bard': 'data/bard_generations.csv',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from marked_words import marked_words\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from konlpy.tag import Mecab\n",
    "import re\n",
    "import sklearn.feature_selection\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint(dic):\n",
    "    full_list = []\n",
    "    for word in sorted(dic,key=lambda x: x[1],reverse=True):\n",
    "        # print(\"%s, %.2f\" % (word[0],word[1]))\n",
    "        full_list.append(word[0])\n",
    "    return full_list\n",
    "\n",
    "def anonymize(bio, replacement=\"\"):\n",
    "    \"\"\"\n",
    "    지역 및 성별을 직접적으로 나타내는 단어를 masking하는 함수\n",
    "    \"\"\"\n",
    "    bio = re.sub(r\"남|여|녀|남자|여자|남성|여성|남편|부인|그|그녀\", replacement, bio)\n",
    "    bio = re.sub(r\"서울|전라도|경상도|제주도|전라|경상|제주\", replacement, bio)\n",
    "    return bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_TOTAL = 0\n",
    "FILTER_IMAGINED = 1   # 자신이 특정 group이라고 생각하고 묘사한 prompt\n",
    "FILTER_DESCRIBE = 2   # 단순 group에 대한 묘사 prompt\n",
    "\n",
    "def get_marked_words(file_path, mask_groups=False, self_imagine_filter=FILTER_TOTAL):\n",
    "    \"\"\"\n",
    "    file_path의 csv 파일에서 marked words의 dictionary를 리턴\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    mw_result = {}\n",
    "\n",
    "    if self_imagine_filter == FILTER_IMAGINED:\n",
    "        df = df.loc[df['prompt_num'] < 3]\n",
    "    elif self_imagine_filter == FILTER_DESCRIBE:\n",
    "        df = df.loc[df['prompt_num'] >= 3]\n",
    "\n",
    "    if mask_groups:\n",
    "        df['text'] = df['text'].apply(anonymize)\n",
    "\n",
    "    # 서울을 majority로 보았을 때, 다른 지역의 marked words\n",
    "    for province in df['province'].unique():\n",
    "        # print('\\n Top words for %s \\n-------' % province)\n",
    "        outs = pprint(marked_words(df, [province], ['province'], ['서울'], threshold=Z_THRESHOLD))\n",
    "        mw_result[province] = outs\n",
    "\n",
    "    # 다른 지역을 각각 majority로 보았을 때, 서울의 marked words\n",
    "    # temps = []\n",
    "    # for province in df[\"province\"].unique():\n",
    "    #     # print('\\n Top words for %s \\n-------' % province)\n",
    "    #     temp = pprint(marked_words(df, [\"서울\"], [\"province\"], [province], threshold=Z_THRESHOLD))\n",
    "    #     temps.extend(temp)\n",
    "    # seen = Counter(temps).most_common()\n",
    "    # mw_result[\"서울\"] = [w for w, c in seen if c == 3]\n",
    "\n",
    "    # 남자를 majority로 보았을 때, 여자의 marked words\n",
    "    for gender in df[\"gender\"].unique():\n",
    "        # print('\\n Top words for %s \\n-------' % gender)\n",
    "        outs = pprint(marked_words(df, [gender], [\"gender\"], [\"남자\"], threshold=Z_THRESHOLD))\n",
    "        mw_result[gender] = outs\n",
    "\n",
    "    # 여자를 majority로 보았을 때, 남자의 marked words\n",
    "    # temps = []\n",
    "    # for gender in df[\"gender\"].unique():\n",
    "    #     # print('\\n Top words for %s \\n-------' % gender)\n",
    "    #     temp = pprint(marked_words(df, [\"남자\"], [\"gender\"], [gender], threshold=Z_THRESHOLD))\n",
    "    #     temps.extend(temp)\n",
    "    # seen = Counter(temps).most_common()\n",
    "    # mw_result[\"남자\"] = [w for w, c in seen if c == 1]\n",
    "\n",
    "    # 서울 남자를 majority로 보았을 때, 다른 intersectional 집단의 marked words\n",
    "    for province in df[\"province\"].unique():\n",
    "        for gen in df[\"gender\"].unique():\n",
    "            mw_result[f\"{province} {gen}\"] = pprint(\n",
    "                marked_words(df, [province, gen], [\"province\", \"gender\"], [\"서울\", \"남자\"], threshold=Z_THRESHOLD)\n",
    "            )\n",
    "\n",
    "    # 다른 intersectional 집단을 majority로 보았을 때, 서울 남자의 marked words\n",
    "    # temps = []\n",
    "    # for province in df[\"province\"].unique():\n",
    "    #     for gender in df[\"gender\"].unique():\n",
    "    #         # print('\\n Top words for %s \\n-------' % gender)\n",
    "    #         temp = pprint(marked_words(df, [\"서울\", \"남자\"], [\"province\", \"gender\"], [province, gender], threshold=Z_THRESHOLD))\n",
    "    #         temps.extend(temp)\n",
    "    # seen = Counter(temps).most_common()\n",
    "    # mw_result[\"서울 남자\"] = [w for w, c in seen if c == 4 * 2 - 1]\n",
    "\n",
    "    return mw_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classification_task(file_path):\n",
    "    \"\"\"\n",
    "    SVM으로 직접적으로 집단에 대한 정보를 제공하는 단어를 masking한 뒤, 각 집단을 분류하는 성능을 측정\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    mecab = Mecab()\n",
    "\n",
    "    vectorizer = CountVectorizer(binary=True, decode_error=\"ignore\")\n",
    "    tokenizer = vectorizer.build_tokenizer()\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"province_gender\"] = df_copy[\"province\"] + df_copy[\"gender\"]\n",
    "    data = (\n",
    "        df_copy[\"text\"]\n",
    "        .apply(lambda s: \" \".join(mecab.morphs(s)))\n",
    "        .str.lower()\n",
    "        .replace(\"[^\\w\\s]\", \"\", regex=True)\n",
    "    )\n",
    "\n",
    "    top_words = dict()\n",
    "    dv3_svm = {}\n",
    "    for st in [\"province\", \"gender\", \"province_gender\"]:\n",
    "        print(st.upper())\n",
    "        concept_data = [anonymize(d) for d in data]\n",
    "        labels = df_copy[st]\n",
    "        bios_data_train, bios_data_test, Y_train, Y_test = train_test_split(\n",
    "            concept_data, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "        )\n",
    "        vectorizer = CountVectorizer(analyzer=\"word\", min_df=0.001, binary=False)\n",
    "        X_train = vectorizer.fit_transform(bios_data_train)\n",
    "        X_test = vectorizer.transform(bios_data_test)\n",
    "        accs = []\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        for r in df_copy[st].unique():\n",
    "            svm = SVC(kernel=\"linear\")\n",
    "            Y_train_bin = Y_train == r\n",
    "            svm.fit(X_train, Y_train_bin)\n",
    "            acc = sklearn.metrics.accuracy_score(Y_test == r, svm.predict(X_test))\n",
    "            # print(\"%s Accuracy: %.2f\"%(r, acc))\n",
    "            accs.append(acc)\n",
    "            coef = svm.coef_.toarray()[0]\n",
    "            _, names = zip(*sorted(zip(coef, feature_names)))\n",
    "            # print(\"Top 10 words: %s\" % str(names[-10:][::-1]))\n",
    "            dv3_svm[r] = names[-10:][::-1]\n",
    "        print(\n",
    "            \"Mean accuracy across %s groups: %.2f ± %.2f\"\n",
    "            % (st, np.mean(accs), np.std(accs))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marked_words_of_models = dict()\n",
    "\n",
    "for model_name, data_path in models.items():\n",
    "    print(f\"Extracting marked words from {model_name}...\")    \n",
    "    mw = get_marked_words(data_path, mask_groups=True, self_imagine_filter=FILTER_TOTAL)\n",
    "    marked_words_of_models[model_name] = mw\n",
    "\n",
    "# Save as a file\n",
    "with open('figures/result.p', 'wb') as f:\n",
    "    pickle.dump(marked_words_of_models, f)\n",
    "\n",
    "# Open a file\n",
    "with open(\"figures/result.p\", 'rb') as f:\n",
    "    marked_words_of_models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(korean):\n",
    "    translation_map = {'서울': 'Seoul', '제주도': 'Jeju', '경상도': 'Gyeong-\\nsang', '전라도': 'Jeolla', '남자': 'a Man', '여자': 'a Woman'}\n",
    "    if len(korean.split()) == 1:\n",
    "        return translation_map[korean]\n",
    "    else:\n",
    "        return f\"{translation_map[korean.split()[1]]}\\nfrom\\n{translation_map[korean.split()[0]]}\"\n",
    "\n",
    "def plot_groups(marked_words_of_models):\n",
    "    # Data\n",
    "    gender = ['남자', '여자']\n",
    "    province = ['서울', '제주도', '경상도', '전라도']\n",
    "    province_gender = []\n",
    "    for g in gender:\n",
    "        for p in province:\n",
    "            province_gender.append(p + \" \" + g)\n",
    "\n",
    "    plot_data = dict()\n",
    "\n",
    "    for g in gender:\n",
    "        plot_data[g] = dict()\n",
    "\n",
    "    for p in province:\n",
    "        plot_data[p] = dict()\n",
    "\n",
    "    for g in gender:\n",
    "        for p in province:\n",
    "            plot_data[p + \" \" + g] = dict()\n",
    "\n",
    "    plot_titles = ['Gender', 'Province', 'Province and Gender']\n",
    "    for title_number, groups in enumerate([gender, province, province_gender]):\n",
    "        plt.figure()\n",
    "        for model, marked_words in marked_words_of_models.items():\n",
    "            for group in marked_words.keys():\n",
    "                plot_data[group][model] = len(marked_words[group])\n",
    "            models = list(plot_data[groups[0]].keys())\n",
    "\n",
    "        bar_width = 0.2\n",
    "        index = np.arange(len(groups))\n",
    "\n",
    "        for i, version in enumerate(models):\n",
    "            plt.bar(index + bar_width * i, [plot_data[category][version] for category in groups], width=bar_width, label=f'{version}')\n",
    "\n",
    "        # Customize the plot with smaller font size\n",
    "        plt.xlabel(f'{plot_titles[title_number]}', fontsize=10)\n",
    "        plt.ylabel('Marked Word Count', fontsize=10)\n",
    "        plt.title(f'Marked Word Count by {plot_titles[title_number]}', fontsize=12)\n",
    "        plt.xticks(index + (bar_width / 2) * (len(models) - 1), list(map(translate, groups)), fontsize=8)\n",
    "        plt.legend(fontsize=8)\n",
    "        plt.savefig(f'figures/result_{plot_titles[title_number]}.png'.lower().replace(\" \", \"_\"))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_groups(marked_words_of_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in marked_words_of_models.keys():\n",
    "    print(f\"{key} : {marked_words_of_models[key]['서울']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
