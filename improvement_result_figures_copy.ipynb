{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marked word 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_THRESHOLD = 1.96 # Threshold for detecting marked words\n",
    "\n",
    "models = {\n",
    "    'gpt-3.5-turbo': 'data/gpt3_generations.csv',\n",
    "    'gpt-4-1106-preview': 'data/gpt4_generations.csv',\n",
    "    'CLOVA X': 'data/clovax_generations.csv',\n",
    "    'Bard': 'data/bard_generations.csv',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from konlpy.tag import Mecab\n",
    "import re\n",
    "import sklearn.feature_selection\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Running this file obtains the words that distinguish a target group from the corresponding\n",
    "unmarked ones.\n",
    "Example usage: (To obtain the words that differentiate the 'Asian F' category)\n",
    "python3 marked_words.py ../generated_personas.csv --target_val 'an Asian' F --target_col race gender --unmarked_val 'a White' M\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import sys\n",
    "from konlpy.tag import Kkma, Mecab\n",
    "\n",
    "kkma, mecab = Kkma(), Mecab()\n",
    "TAGS = ['NNG', 'NNP', 'VV', 'VA', 'VX', 'MM', 'MAG', 'XR']\n",
    "\n",
    "def preprocess(s):\n",
    "    return \" \".join(list(map(lambda p: p[0], filter(lambda x: x[1].split(\"+\")[0] in TAGS, mecab.pos(s)))))\n",
    "\n",
    "def get_log_odds(df1, df2, df0, verbose=False, lower=True):\n",
    "    \"\"\"Monroe et al. Fightin' Words method to identify top words in df1 and df2\n",
    "    against df0 as the background corpus\"\"\"\n",
    "    counts1 = defaultdict(\n",
    "        int,\n",
    "        [\n",
    "            [i, j]\n",
    "            for i, j in df1.apply(preprocess)\n",
    "            .str.lower()\n",
    "            .str.split(expand=True)\n",
    "            .stack()\n",
    "            .replace(\"[\\W]\", \"\", regex=True)\n",
    "            .value_counts()\n",
    "            .items()\n",
    "        ],\n",
    "    )\n",
    "    counts2 = defaultdict(\n",
    "        int,\n",
    "        [\n",
    "            [i, j]\n",
    "            for i, j in df2.apply(preprocess)\n",
    "            .str.lower()\n",
    "            .str.split(expand=True)\n",
    "            .stack()\n",
    "            .replace(\"[\\W]\", \"\", regex=True)\n",
    "            .value_counts()\n",
    "            .items()\n",
    "        ],\n",
    "    )\n",
    "    prior = defaultdict(\n",
    "        int,\n",
    "        [\n",
    "            [i, j]\n",
    "            for i, j in df0.apply(preprocess)\n",
    "            .str.lower()\n",
    "            .str.split(expand=True)\n",
    "            .stack()\n",
    "            .replace(\"[\\W]\", \"\", regex=True)\n",
    "            .value_counts()\n",
    "            .items()\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    sigmasquared = defaultdict(float)\n",
    "    sigma = defaultdict(float)\n",
    "    delta = defaultdict(float)\n",
    "\n",
    "    for word in prior.keys():\n",
    "        prior[word] = int(prior[word] + 0.5)\n",
    "\n",
    "    for word in counts2.keys():\n",
    "        counts1[word] = int(counts1[word] + 0.5)\n",
    "        if prior[word] == 0:\n",
    "            prior[word] = 1\n",
    "\n",
    "    for word in counts1.keys():\n",
    "        counts2[word] = int(counts2[word] + 0.5)\n",
    "        if prior[word] == 0:\n",
    "            prior[word] = 1\n",
    "\n",
    "    n1 = sum(counts1.values())\n",
    "    n2 = sum(counts2.values())\n",
    "    nprior = sum(prior.values())\n",
    "\n",
    "    for word in prior.keys():\n",
    "        if prior[word] > 0:\n",
    "            l1 = float(counts1[word] + prior[word]) / (\n",
    "                (n1 + nprior) - (counts1[word] + prior[word])\n",
    "            )\n",
    "            l2 = float(counts2[word] + prior[word]) / (\n",
    "                (n2 + nprior) - (counts2[word] + prior[word])\n",
    "            )\n",
    "            sigmasquared[word] = 1 / (float(counts1[word]) + float(prior[word])) + 1 / (\n",
    "                float(counts2[word]) + float(prior[word])\n",
    "            )\n",
    "            sigma[word] = math.sqrt(sigmasquared[word])\n",
    "            delta[word] = (math.log(l1) - math.log(l2)) / sigma[word]\n",
    "\n",
    "    if verbose:\n",
    "        for word in sorted(delta, key=delta.get)[:10]:\n",
    "            print(\"%s, %.3f\" % (word, delta[word]))\n",
    "\n",
    "        for word in sorted(delta, key=delta.get, reverse=True)[:10]:\n",
    "            print(\"%s, %.3f\" % (word, delta[word]))\n",
    "\n",
    "    return delta\n",
    "\n",
    "\n",
    "def marked_words(df, target_val, target_col, unmarked_val, corpus=None, threshold=1.96, verbose=False):\n",
    "\n",
    "    \"\"\"Get words that distinguish the target group (which is defined as having\n",
    "    target_group_vals in the target_group_cols column of the dataframe)\n",
    "    from all unmarked_attrs (list of values that correspond to the categories\n",
    "    in unmarked_attrs)\"\"\"\n",
    "\n",
    "    grams = dict()\n",
    "    thr = threshold  # z-score threshold\n",
    "\n",
    "    if corpus is None:\n",
    "        corpus = df\n",
    "\n",
    "    subdf = df.copy()\n",
    "\n",
    "    for i in range(len(target_val)):\n",
    "        subdf = subdf.loc[subdf[target_col[i]] == target_val[i]]\n",
    "\n",
    "    for i in range(len(unmarked_val)):\n",
    "        delt = get_log_odds(\n",
    "            subdf[\"text\"],\n",
    "            df[\"text\"],  # df.loc[df[target_col[i]] == unmarked_val[i]][\"text\"],\n",
    "            corpus[\"text\"],\n",
    "            verbose,\n",
    "        )  # first one is the positive-valued one\n",
    "        c1 = []\n",
    "        c2 = []\n",
    "\n",
    "        for k, v in delt.items():\n",
    "            if v > thr:\n",
    "                c1.append([k, v])\n",
    "            elif v < -thr:\n",
    "                c2.append([k, v])\n",
    "\n",
    "        if \"target\" in grams:\n",
    "            grams[\"target\"].extend(c1)\n",
    "        else:\n",
    "            grams[\"target\"] = c1\n",
    "\n",
    "        # if unmarked_val[i] in grams:\n",
    "        #     grams[unmarked_val[i]].extend(c2)\n",
    "        # else:\n",
    "        #     grams[unmarked_val[i]] = c2\n",
    "\n",
    "    grams_refine = dict()\n",
    "\n",
    "    for r in grams.keys():\n",
    "        temp = []\n",
    "        thr = len(unmarked_val)  # must satisfy all intersections\n",
    "        for k, v in Counter([word for word, z in grams[r]]).most_common():\n",
    "            if v >= thr:\n",
    "                z_score_sum = np.sum([z for word, z in grams[r] if word == k])\n",
    "                temp.append([k, z_score_sum])\n",
    "\n",
    "        grams_refine[r] = temp\n",
    "\n",
    "    return grams_refine[\"target\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint(dic):\n",
    "    full_list = []\n",
    "    for word in sorted(dic,key=lambda x: x[1],reverse=True):\n",
    "        # print(\"%s, %.2f\" % (word[0],word[1]))\n",
    "        full_list.append(word[0])\n",
    "    return full_list\n",
    "\n",
    "def anonymize(bio, replacement=\"\"):\n",
    "    \"\"\"\n",
    "    지역 및 성별을 직접적으로 나타내는 단어를 masking하는 함수\n",
    "    \"\"\"\n",
    "    bio = re.sub(r\"남|여|녀|남자|여자|남성|여성|남편|부인|그|그녀\", replacement, bio)\n",
    "    bio = re.sub(r\"서울|전라도|경상도|제주도|전라|경상|제주\", replacement, bio)\n",
    "    return bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_TOTAL = 0\n",
    "FILTER_IMAGINED = 1   # 자신이 특정 group이라고 생각하고 묘사한 prompt\n",
    "FILTER_DESCRIBE = 2   # 단순 group에 대한 묘사 prompt\n",
    "\n",
    "def get_marked_words(file_path, mask_groups=False, self_imagine_filter=FILTER_TOTAL):\n",
    "    \"\"\"\n",
    "    file_path의 csv 파일에서 marked words의 dictionary를 리턴\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    mw_result = {}\n",
    "\n",
    "    if self_imagine_filter == FILTER_IMAGINED:\n",
    "        df = df.loc[df['prompt_num'] < 3]\n",
    "    elif self_imagine_filter == FILTER_DESCRIBE:\n",
    "        df = df.loc[df['prompt_num'] >= 3]\n",
    "\n",
    "    if mask_groups:\n",
    "        df['text'] = df['text'].apply(anonymize)\n",
    "\n",
    "    # 서울을 majority로 보았을 때, 다른 지역의 marked words\n",
    "    for province in df['province'].unique():\n",
    "        # print('\\n Top words for %s \\n-------' % province)\n",
    "        outs = marked_words(df, [province], ['province'], ['서울'], threshold=Z_THRESHOLD)\n",
    "        mw_result[province] = outs\n",
    "\n",
    "    # 다른 지역을 각각 majority로 보았을 때, 서울의 marked words\n",
    "    # temps = []\n",
    "    # for province in df[\"province\"].unique():\n",
    "    #     # print('\\n Top words for %s \\n-------' % province)\n",
    "    #     temp = pprint(marked_words(df, [\"서울\"], [\"province\"], [province], threshold=Z_THRESHOLD))\n",
    "    #     temps.extend(temp)\n",
    "    # seen = Counter(temps).most_common()\n",
    "    # mw_result[\"서울\"] = [w for w, c in seen if c == 3]\n",
    "\n",
    "    # 남자를 majority로 보았을 때, 여자의 marked words\n",
    "    for gender in df[\"gender\"].unique():\n",
    "        # print('\\n Top words for %s \\n-------' % gender)\n",
    "        outs = marked_words(df, [gender], [\"gender\"], [\"남자\"], threshold=Z_THRESHOLD)\n",
    "        mw_result[gender] = outs\n",
    "\n",
    "    # 여자를 majority로 보았을 때, 남자의 marked words\n",
    "    # temps = []\n",
    "    # for gender in df[\"gender\"].unique():\n",
    "    #     # print('\\n Top words for %s \\n-------' % gender)\n",
    "    #     temp = pprint(marked_words(df, [\"남자\"], [\"gender\"], [gender], threshold=Z_THRESHOLD))\n",
    "    #     temps.extend(temp)\n",
    "    # seen = Counter(temps).most_common()\n",
    "    # mw_result[\"남자\"] = [w for w, c in seen if c == 1]\n",
    "\n",
    "    # 서울 남자를 majority로 보았을 때, 다른 intersectional 집단의 marked words\n",
    "    for province in df[\"province\"].unique():\n",
    "        for gen in df[\"gender\"].unique():\n",
    "            mw_result[f\"{province} {gen}\"] = \\\n",
    "                marked_words(df, [province, gen], [\"province\", \"gender\"], [\"서울\", \"남자\"], threshold=Z_THRESHOLD)\n",
    "\n",
    "    # 다른 intersectional 집단을 majority로 보았을 때, 서울 남자의 marked words\n",
    "    # temps = []\n",
    "    # for province in df[\"province\"].unique():\n",
    "    #     for gender in df[\"gender\"].unique():\n",
    "    #         # print('\\n Top words for %s \\n-------' % gender)\n",
    "    #         temp = pprint(marked_words(df, [\"서울\", \"남자\"], [\"province\", \"gender\"], [province, gender], threshold=Z_THRESHOLD))\n",
    "    #         temps.extend(temp)\n",
    "    # seen = Counter(temps).most_common()\n",
    "    # mw_result[\"서울 남자\"] = [w for w, c in seen if c == 4 * 2 - 1]\n",
    "\n",
    "    return mw_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classification_task(file_path):\n",
    "    \"\"\"\n",
    "    SVM으로 직접적으로 집단에 대한 정보를 제공하는 단어를 masking한 뒤, 각 집단을 분류하는 성능을 측정\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    mecab = Mecab()\n",
    "\n",
    "    vectorizer = CountVectorizer(binary=True, decode_error=\"ignore\")\n",
    "    tokenizer = vectorizer.build_tokenizer()\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"province_gender\"] = df_copy[\"province\"] + df_copy[\"gender\"]\n",
    "    data = (\n",
    "        df_copy[\"text\"]\n",
    "        .apply(lambda s: \" \".join(mecab.morphs(s)))\n",
    "        .str.lower()\n",
    "        .replace(\"[^\\w\\s]\", \"\", regex=True)\n",
    "    )\n",
    "\n",
    "    top_words = dict()\n",
    "    dv3_svm = {}\n",
    "    for st in [\"province\", \"gender\", \"province_gender\"]:\n",
    "        print(st.upper())\n",
    "        concept_data = [anonymize(d) for d in data]\n",
    "        labels = df_copy[st]\n",
    "        bios_data_train, bios_data_test, Y_train, Y_test = train_test_split(\n",
    "            concept_data, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "        )\n",
    "        vectorizer = CountVectorizer(analyzer=\"word\", min_df=0.001, binary=False)\n",
    "        X_train = vectorizer.fit_transform(bios_data_train)\n",
    "        X_test = vectorizer.transform(bios_data_test)\n",
    "        accs = []\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        for r in df_copy[st].unique():\n",
    "            svm = SVC(kernel=\"linear\")\n",
    "            Y_train_bin = Y_train == r\n",
    "            svm.fit(X_train, Y_train_bin)\n",
    "            acc = sklearn.metrics.accuracy_score(Y_test == r, svm.predict(X_test))\n",
    "            # print(\"%s Accuracy: %.2f\"%(r, acc))\n",
    "            accs.append(acc)\n",
    "            coef = svm.coef_.toarray()[0]\n",
    "            _, names = zip(*sorted(zip(coef, feature_names)))\n",
    "            # print(\"Top 10 words: %s\" % str(names[-10:][::-1]))\n",
    "            dv3_svm[r] = names[-10:][::-1]\n",
    "        print(\n",
    "            \"Mean accuracy across %s groups: %.2f ± %.2f\"\n",
    "            % (st, np.mean(accs), np.std(accs))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting marked words from gpt-3.5-turbo...\n",
      "Extracting marked words from gpt-4-1106-preview...\n",
      "Extracting marked words from CLOVA X...\n",
      "Extracting marked words from Bard...\n"
     ]
    }
   ],
   "source": [
    "marked_words_of_models = dict()\n",
    "\n",
    "for model_name, data_path in models.items():\n",
    "    print(f\"Extracting marked words from {model_name}...\")    \n",
    "    mw = get_marked_words(data_path, mask_groups=True, self_imagine_filter=FILTER_TOTAL)\n",
    "    marked_words_of_models[model_name] = mw\n",
    "\n",
    "# Save as a file\n",
    "with open('figures/result_z_score.p', 'wb') as f:\n",
    "    pickle.dump(marked_words_of_models, f)\n",
    "\n",
    "# Open a file\n",
    "with open(\"figures/result_z_score.p\", 'rb') as f:\n",
    "    marked_words_of_models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(korean):\n",
    "    translation_map = {'서울': 'Seoul', '제주도': 'Jeju', '경상도': 'Gyeong-\\nsang', '전라도': 'Jeolla', '남자': 'a Man', '여자': 'a Woman'}\n",
    "    if len(korean.split()) == 1:\n",
    "        return translation_map[korean]\n",
    "    else:\n",
    "        return f\"{translation_map[korean.split()[1]]}\\nfrom\\n{translation_map[korean.split()[0]]}\"\n",
    "\n",
    "def plot_groups(marked_words_of_models):\n",
    "    # Data\n",
    "    gender = ['남자', '여자']\n",
    "    province = ['서울', '제주도', '경상도', '전라도']\n",
    "    province_gender = []\n",
    "    for g in gender:\n",
    "        for p in province:\n",
    "            province_gender.append(p + \" \" + g)\n",
    "\n",
    "    plot_data = dict()\n",
    "\n",
    "    for g in gender:\n",
    "        plot_data[g] = dict()\n",
    "\n",
    "    for p in province:\n",
    "        plot_data[p] = dict()\n",
    "\n",
    "    for g in gender:\n",
    "        for p in province:\n",
    "            plot_data[p + \" \" + g] = dict()\n",
    "\n",
    "    plot_titles = ['Gender', 'Region', 'Region and Gender']\n",
    "    for title_number, groups in enumerate([gender, province, province_gender]):\n",
    "        plt.figure()\n",
    "        for model, marked_words in marked_words_of_models.items():\n",
    "            for group in marked_words.keys():\n",
    "                plot_data[group][model] = len(map(lambda x: x[1], marked_words[group]))\n",
    "            models = list(plot_data[groups[0]].keys())\n",
    "\n",
    "        bar_width = 0.2\n",
    "        index = np.arange(len(groups))\n",
    "\n",
    "        for i, version in enumerate(models):\n",
    "            plt.bar(index + bar_width * i, [plot_data[category][version] for category in groups], width=bar_width, label=f'{version}')\n",
    "\n",
    "        # Customize the plot with smaller font size\n",
    "        plt.xlabel(f'{plot_titles[title_number]}', fontsize=10)\n",
    "        plt.ylabel('Marked Word Count', fontsize=10)\n",
    "        plt.title(f'Marked Word Count by {plot_titles[title_number]} (Sum of z-score)', fontsize=12)\n",
    "        plt.xticks(index + (bar_width / 2) * (len(models) - 1), list(map(translate, groups)), fontsize=8)\n",
    "        plt.legend(fontsize=8)\n",
    "        plt.savefig(f'figures/result_{plot_titles[title_number]}_z_score.png'.lower().replace(\" \", \"_\"))\n",
    "        plt.show()\n",
    "\n",
    "    for model, marked_words in marked_words_of_models.items():\n",
    "        for group in marked_words.keys():\n",
    "            plot_data[group][model] = len(list(map(lambda x: x[1], marked_words[group])))\n",
    "        print(f'{model} 성별 : {plot_data[\"남자\"][model] + plot_data[\"여자\"][model]}')\n",
    "        print(f'{model} 지역 : {plot_data[\"서울\"][model] + plot_data[\"제주도\"][model] + plot_data[\"전라도\"][model] + plot_data[\"경상도\"][model]}')\n",
    "        print(f'{model} 성별 지역 : {plot_data[\"서울 남자\"][model] + plot_data[\"제주도 남자\"][model] + plot_data[\"전라도 남자\"][model] + plot_data[\"경상도 남자\"][model] + plot_data[\"서울 여자\"][model] + plot_data[\"제주도 여자\"][model] + plot_data[\"전라도 여자\"][model] + plot_data[\"경상도 여자\"][model]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'map' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_40962/1958786614.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarked_words_of_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_40962/2555268579.py\u001b[0m in \u001b[0;36mplot_groups\u001b[0;34m(marked_words_of_models)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarked_words\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmarked_words_of_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmarked_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mplot_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarked_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'map' has no len()"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_groups(marked_words_of_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- gpt-3.5-turbo --\n",
      "[['의', 5.322980873111288], ['자연', 4.603614419488489], ['바다', 4.515691806521256], ['햇살', 3.3135695566360877], ['자유', 3.155294168396375], ['섬', 3.06306309993021], ['바람', 3.017446688234478], ['서핑', 3.013991526620775], ['해변', 2.782371383005907], ['아름다움', 2.724423753171871], ['아름다운', 2.7119511566560286], ['파도', 2.6452852508556215], ['영혼', 2.614634577404875], ['푸른', 2.4474874886987568], ['해안', 2.410320486843273], ['맑', 2.2277108268513057], ['조화', 2.073457445562407], ['곳', 1.998655730828549]]\n",
      "-- gpt-4-1106-preview --\n",
      "[['바다', 7.095534905151349], ['의', 6.980138430582396], ['자연', 6.975519785267767], ['바람', 5.082284826964877], ['섬', 4.708953856944759], ['해', 3.932187278855327], ['감귤', 3.706350124537093], ['파도', 3.4103835095317994], ['피부', 3.3136833784661106], ['삶', 3.081859793052525], ['물질', 2.752384216923007], ['햇볕', 2.7411441857283134], ['해산물', 2.7175294681964965], ['푸른', 2.664530675484721], ['을', 2.5114385398426315], ['오름', 2.4182449153237826], ['거친', 2.4123852872130906], ['전통', 2.3774304665304826], ['강인', 2.3542758143109404], ['린', 2.3349587405249856], ['조화', 2.3227702491084177], ['밭', 2.2959023738107236], ['아름다움', 2.2308181728029384], ['관광객', 2.2243050180822586], ['맑', 2.223421296753733], ['튼튼', 2.1962907298569476], ['건강', 2.1837612438483465], ['평화', 2.1769684144697146], ['환경', 2.165406579720257], ['마을', 2.127983223913851], ['채취', 2.120716892950291], ['해안가', 2.06698272553299], ['자연환경', 2.06698272553299], ['해변', 2.0501836598280088], ['닮', 2.0203020747118883], ['소박', 2.0088211059743974], ['함께', 2.00132898983537], ['하늘', 1.9692721156586948], ['체격', 1.9668044027781515], ['어업', 1.9664622479506757]]\n",
      "-- CLOVA X --\n",
      "[['의', 9.325038696692582], ['자연', 5.926517876557698], ['문화', 5.723680898788126], ['전통', 4.295367296583231], ['아름다운', 4.126821630440159], ['바다', 4.0151106932981016], ['보존', 3.76222242102821], ['자연환경', 3.3965948047752397], ['사랑', 3.14826085844934], ['산', 3.1209682428375087], ['발전', 2.6083017316133192], ['밝', 2.3934191558473383], ['살아가', 2.361882364533702], ['인내심', 2.3477200599639354], ['강인', 2.283743630746271], ['바람', 2.283743630746271], ['에서', 2.247283767476448], ['환경', 2.190153185832349], ['돼지', 2.1863858833682075], ['어우러진', 2.136052856917029], ['태어나', 2.080781432933066], ['역사', 2.0784991605001704], ['자란', 2.0428116399587024]]\n",
      "-- Bard --\n",
      "[['자연', 8.12702790651396], ['의', 6.925994264364431], ['아름다운', 5.732203117141394], ['바다', 5.731810779272488], ['푸른', 4.2165173946133265], ['지키', 3.071644283984865], ['따뜻', 2.8697394641406766], ['섬', 2.7027588841231442], ['를', 2.593558933441062], ['하늘', 2.58625188227451], ['척박', 2.565049909159694], ['속', 2.5571830538409683], ['한라산', 2.495899870158947], ['사랑', 2.4591020447018197], ['환경', 2.3696492429668328], ['자연환경', 2.3063982882953455], ['독특', 2.296643874608276], ['아름다움', 2.197698153885796], ['미래', 2.1619924365187377], ['강인', 2.1565884414160217], ['밭', 2.1236078291622857], ['함께', 2.1220229238522554], ['자란', 2.1137021540819076], ['농사', 2.046618472023314], ['섬세', 2.0396947843586455], ['잠수', 2.019880144797001], ['햇살', 2.013773582663783], ['마음', 2.009483922893792], ['건강', 2.001155337684831], ['맑', 1.9618631382277332]]\n"
     ]
    }
   ],
   "source": [
    "for model in models.keys():\n",
    "    print(f\"-- {model} --\")\n",
    "    marked_words_of_models[model]['제주도'].sort(key=lambda x : x[1], reverse=True)\n",
    "    print(marked_words_of_models[model]['제주도'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
